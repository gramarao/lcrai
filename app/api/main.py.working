from fastapi import FastAPI, Depends, HTTPException, UploadFile, File, APIRouter
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.sql import text
from datetime import datetime
import time
import json
import logging
from app.models.database import SessionLocal, Document, DocumentChunk
from app.services.rag_service_googleai import rag_service
from config.settings import settings

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = FastAPI(title="RAG API", version="1.0.0")
router = APIRouter(prefix="/api")

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

start_time = time.time()

@router.get("/health")
async def health_check(db: Session = Depends(get_db)):
    try:
        # Combine queries for efficiency
        result = db.execute(text("""
            SELECT 
                (SELECT COUNT(*) FROM documents) AS doc_count,
                (SELECT COUNT(*) FROM document_chunks WHERE embedding IS NOT NULL) AS chunk_count,
                pg_size_pretty(pg_total_relation_size('document_chunks')) AS chunk_size
        """)).first()
        
        doc_count, chunk_count, chunk_size = result if result else (0, 0, "0 B")
        logger.debug(f"Health check: {doc_count} docs, {chunk_count} chunks, size {chunk_size}")
        
        return {
            "status": "healthy",
            "service": "RAG",
            "timestamp": datetime.now().isoformat(),
            "document_count": doc_count,
            "chunk_count": chunk_count,
            "embedding_storage": chunk_size or "0 B",
            "uptime": time.time() - start_time
        }
    except SQLAlchemyError as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

@router.get("/documents")
async def get_documents(db: Session = Depends(get_db)):
    try:
        docs = db.query(Document).all()
        result = []
        for doc in docs:
            chunk_count = db.query(DocumentChunk).filter(DocumentChunk.document_id == doc.id).count()
            avg_chunk_size = db.execute(text("""
                SELECT COALESCE(AVG(LENGTH(content)), 0)
                FROM document_chunks
                WHERE document_id = :doc_id
            """), {"doc_id": doc.id}).scalar() or 0
            result.append({
                "id": str(doc.id),
                "filename": doc.filename,
                "chunk_count": chunk_count,
                "content_length": len(doc.content) if doc.content else 0,
                "avg_chunk_size": float(avg_chunk_size),
                "created_at": doc.created_at.isoformat() if doc.created_at else None,
                "status": "existing"
            })
        logger.info(f"Fetched {len(result)} documents")
        return {"documents": result, "total": len(result)}
    except Exception as e:
        logger.error(f"Failed to fetch documents: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch documents: {str(e)}")
    finally:
        db.close()

@router.post("/query")
async def query_endpoint(request: dict, db: Session = Depends(get_db)):
    question = request.get("question", "").strip()
    selected_docs = request.get("selected_docs", [])
    max_sources = request.get("max_sources", 3)
    
    if not question:
        raise HTTPException(status_code=400, detail="Question cannot be empty")
    
    try:
        doc_ids = None
        if selected_docs:
            doc_ids = db.query(Document.id).filter(Document.filename.in_(selected_docs)).all()
            doc_ids = [str(doc_id[0]) for doc_id in doc_ids]
            if not doc_ids:
                logger.warning(f"No documents found for selected filenames: {selected_docs}")
                return {
                    "answer": "No matching documents found for the selected filenames.",
                    "sources": [],
                    "response_time": 0,
                    "tokens_used": 0
                }
        
        result = await rag_service.search_and_generate(question, max_sources=max_sources, doc_ids=doc_ids)
        logger.info(f"Query processed: {question[:50]}...")
        return result
    except Exception as e:
        logger.error(f"Query failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")

# In app/api/main.py
@router.post("/query_stream")
async def query_stream_endpoint(request: dict, db: Session = Depends(get_db)):
    question = request.get("question", "").strip()
    selected_docs = request.get("selected_docs", [])
    max_sources = request.get("max_sources", 3)
    
    start_time = time.time()
    logger.debug(f"Query stream start: {question[:50]}...")
    
    if not question:
        async def error_stream():
            yield json.dumps({
                "type": "error",
                "message": "Question cannot be empty",
                "is_complete": True
            }) + "\n"
        return StreamingResponse(error_stream(), media_type="text/event-stream")
    
    async def event_stream():
        try:
            logger.debug(f"Fetching doc_ids: {selected_docs}")
            doc_start = time.time()
            doc_ids = None
            if selected_docs:
                doc_ids = db.query(Document.id).filter(Document.filename.in_(selected_docs)).all()
                doc_ids = [str(doc_id[0]) for doc_id in doc_ids]
                logger.debug(f"Resolved doc_ids: {doc_ids} in {time.time() - doc_start:.2f}s")
                if not doc_ids:
                    yield json.dumps({
                        "type": "complete",
                        "full_response": "No matching documents found for the selected filenames.",
                        "sources": [],
                        "response_time": time.time() - start_time,
                        "tokens_used": 0,
                        "is_complete": True
                    }) + "\n"
                    return
            
            logger.debug("Calling stream_search_and_generate...")
            async for chunk_data in rag_service.stream_search_and_generate(
                question, max_sources=max_sources, doc_ids=doc_ids
            ):
                yield json.dumps(chunk_data) + "\n"
                logger.debug(f"Stream chunk: {chunk_data.get('type')} in {time.time() - start_time:.2f}s")
        except Exception as e:
            logger.error(f"Streaming endpoint error: {str(e)}")
            yield json.dumps({
                "type": "error",
                "message": f"Stream failed: {str(e)}",
                "is_complete": True
            }) + "\n"
            yield json.dumps({
                "type": "stream_end",
                "timestamp": datetime.now().isoformat()
            }) + "\n"
    
    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

@router.post("/upload")
async def upload_document(file: UploadFile = File(...), db: Session = Depends(get_db)):
    try:
        content = await file.read()
        result = rag_service.store_document(file.filename, content)
        logger.info(f"Uploaded document: {file.filename}")
        return {
            "status": "new",
            "message": f"Document '{file.filename}' uploaded successfully",
            "document_id": result,
            "filename": file.filename,
            "chunk_count": db.query(DocumentChunk).filter(DocumentChunk.document_id == result).count()
        }
    except Exception as e:
        logger.error(f"Upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")
    finally:
        db.close()

@router.delete("/documents/{doc_id}")
async def delete_document(doc_id: str, db: Session = Depends(get_db)):
    try:
        doc = db.query(Document).filter(Document.id == doc_id).first()
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        chunk_count = db.query(DocumentChunk).filter(DocumentChunk.document_id == doc_id).count()
        db.delete(doc)
        db.commit()
        logger.info(f"Deleted document {doc_id} with {chunk_count} chunks")
        return {
            "success": True,
            "message": f"Deleted document '{doc.filename}' and {chunk_count} chunks",
            "deleted_id": doc_id
        }
    except Exception as e:
        db.rollback()
        logger.error(f"Delete failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Delete failed: {str(e)}")
    finally:
        db.close()

app.include_router(router)